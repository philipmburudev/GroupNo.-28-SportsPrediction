# -*- coding: utf-8 -*-
"""GroupNo.28 SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U655A3z-3uwtkSIvb844Yr4cmF0Aihzl
"""


import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split


print(sklearn.__version__)


#DATA COLLECTION AND READING

fifa_data=pd.read_csv('/Users/philipmburu/Desktop/players_21.csv')
fifa_data

fifa_data.describe()




#PROCESSING THE DATA
#1. Identify columns that affect player rating and that don't


#DROPPING COLUMNS THAT DON'T AFFECT PLAYER RATING

#Columns that don't affect player rating
unwanted_player_data = ['sofifa_id', 'player_url', 'short_name', 'long_name', 'player_face_url', 'club_logo_url',
                    'club_flag_url', 'nation_logo_url', 'nation_flag_url' ]



#NEW DATA AFTER REMOVING THE UNWANTED COLUMNS
new_fifa_data= fifa_data.drop(unwanted_player_data, axis = 1)
new_fifa_data

#IDENTIFY CATEGORICAL VALUES AND ENCODE THEM

Categorical_values = new_fifa_data.select_dtypes(include=['object', 'category']).columns
Categorical_values

#Retrieve data from this categorical values - preferred_foot
categorical_values = new_fifa_data['preferred_foot'].unique()
for value in categorical_values:
    print(value)

#work_rate categorical values
categorical_values = new_fifa_data['work_rate'].unique()
for value in categorical_values:
    print(value)

#body_type categorical values
categorical_values = new_fifa_data['body_type'].unique()
for value in categorical_values:
    print(value)



#REPRESENTATION OF CATEGORICAL DATA IN NUMERICAL FORM
#The dummies
fifa_categorical_data = pd.get_dummies(new_fifa_data, columns=['preferred_foot', 'body_type', 'work_rate'])
fifa_categorical_data



#COLUMNS THAT AFFECT PLAYER PERFORMANCE
wanted_player_data = ['overall','potential', 'passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']
fifa_data[wanted_player_data]



#REMOVE THE CATEGORIAL DATA FROM THE WANTED PLAYER TABLE

# Assuming you have a DataFrame 'fifa_data' and a list of categorical columns to be removed 'categorical_to_be_removed'
categorical_to_be_removed = ['preferred_foot', 'body_type', 'work_rate']


# Create a new DataFrame with categorical columns removed
wanted_player_data_categorical_removed = fifa_data[wanted_player_data].drop(categorical_to_be_removed, axis=1)
wanted_player_data_categorical_removed



#2. MAXIMUM CORRELATION WITH THE DEPENDENT VARIABLE
correlation_matrix21 = fifa_categorical_data.corr()
correlation_matrix21

# Select the dependent variables
dependent_variables = ['passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']


# Calculate the correlation matrix
corr_matrix = fifa_data[dependent_variables].corr()

# Find the maximum correlation coefficient
max_correlation = corr_matrix.max()

# Find the pair of dependent variables with the maximum correlation
max_correlation_pair = corr_matrix.idxmax()

# Print the results
print('Maximum correlation coefficienT:', max_correlation)
print('The pair of dependent variables with the maximum correlation is:', max_correlation_pair)



#3. CREATE AND TRAIN A SUITABLE MACHINE LEARNING MODEL WITH CROSS-VALIDATION THAT CAN PREDICT A PLAYER'S RATING.


#Dropping NAN

# Checking for missing values in the selected columns
fifa_wanted_player_data = fifa_data[wanted_player_data]
missing_values = fifa_wanted_player_data.isnull().sum()
missing_values

#FILLING THE NAN VALUES FOR WANTED PLAYER DATA.
fifa_wanted_player_data.fillna(method='ffill',inplace=True)

fifa_wanted_player_data.fillna(method='bfill',inplace =True)

fifa_wanted_player_data



#RETRIEVE WANTED PLAYER DATA INCLUDING THE CATEGORICAL VARIABLES WHICH HAVE BEEN ENCODED

# Assuming you have a DataFrame 'fifa_wanted_player_data' with columns 'preferred_foot', 'body_type', and 'work_rate'

# Select the columns you want to one-hot encode
fifa_wanted_player_data1 = pd.get_dummies(fifa_wanted_player_data['preferred_foot'], prefix='preferred_foot')
fifa_wanted_player_data2 = pd.get_dummies(fifa_wanted_player_data['body_type'], prefix='body_type')
fifa_wanted_player_data3 = pd.get_dummies(fifa_wanted_player_data['work_rate'], prefix='work_rate')



# Concatenate the one-hot encoded DataFrames with the original DataFrame
fifa_wanted_player_data4 = pd.concat([wanted_player_data_categorical_removed , fifa_wanted_player_data1, fifa_wanted_player_data2, fifa_wanted_player_data3], axis=1)

# Now, 'fifa_wanted_player_data' contains the original data and one-hot encoded columns for 'preferred_foot', 'body_type', and 'work_rate'.


#Print the DataFrame
fifa_wanted_player_data4



# Import the necessary libraries
from sklearn.preprocessing import StandardScaler

non_hot_encoded_columns = ['potential','passing','defending','shooting','international_reputation', 'dribbling']
scalable_data = fifa_wanted_player_data4[non_hot_encoded_columns]



#OBTAINING Y AND X VALUES

from sklearn.preprocessing import StandardScaler

# One-hot encode the categorical columns

# Scaling the independent variables after one-hot encoding
x_values = StandardScaler().fit_transform(scalable_data)

# You can create a DataFrame from the scaled values if needed
x_values = pd.DataFrame(x_values, columns=x_values.columns)

# Check the information of x_values
scaled = pd.DataFrame(x_values, columns=scalable_data.columns)


# Test splitting
scaled.head()

fifa_wanted_player_data4.drop(non_hot_encoded_columns, axis=1, inplace=True)
fifa_wanted_player_data4.head()

dataset = pd.concat([scaled, fifa_wanted_player_data4], axis=1)
dataset.head()

y_values = dataset.overall
x_values = dataset.drop('overall', axis = 1)

x_values = x_values.fillna(x_values.mean())

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Define your RandomForestRegressor and XGBoost models with hyperparameters
randomf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)

# Split the data
Xtrain, Xtest, Ytrain, Ytest = train_test_split(x_values, y_values, test_size=0.2, random_state=42)



# RANDOMFORESTREGRESSOR
# Fit the RandomForestRegressor model
randomf.fit(Xtrain, Ytrain)

# Predict using the model
y_pred_rf = randomf.predict(Xtest)

# Calculate mean absolute error (MAE) for the RandomForestRegressor
mae_rf = mean_absolute_error(y_pred_rf, Ytest)

# Cross-validation for RandomForestRegressor
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
mae_scores_rf = -cross_val_score(randomf, x_values, y_values, cv=kfold, scoring='neg_mean_absolute_error')




# XGBOOST
# Fit the XGBoost model
xgb_model.fit(Xtrain, Ytrain)

# Predict using the XGBoost model
y_pred_xgb = xgb_model.predict(Xtest)

# Calculate MAE for XGBoost
mae_xgb = mean_absolute_error(y_pred_xgb, Ytest)

# Cross-validation for XGBoost
scores_xgb = cross_val_score(xgb_model, x_values, y_values, cv=5, scoring='neg_mean_squared_error')

# Print the MAE and other results
print(f"RandomForestRegressor MAE: {mae_rf}")
print(f"RandomForestRegressor Cross-Validation MAE: {mae_scores_rf.mean()}")
print(f"XGBoost MAE: {mae_xgb}")
print(f"XGBoost Cross-Validation MSE: {-scores_xgb.mean()}")

#4. MEASURE THE MODELâ€™S PERFORMANCE AND FINE-TUNE IT AS A PROCESS OF OPTIMIZATION
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_absolute_error

# Split the data
Xtrain, Xtest, Ytrain, Ytest = train_test_split(x_values, y_values, test_size=0.2, random_state=42)

# Define your RandomForestRegressor model
randomf = RandomForestRegressor(random_state=42)

# Define a range of hyperparameters to search through
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15]
}

# Use GridSearchCV to find the best combination of hyperparameters
grid_search = GridSearchCV(estimator=randomf, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
grid_search.fit(Xtrain, Ytrain)

# Get the best model with tuned hyperparameters
best_model = grid_search.best_estimator_

# Make predictions with the best model
y_pred_best = best_model.predict(Xtest)

# Calculate mean absolute error (MAE) for the best model
mae_best = mean_absolute_error(y_pred_best, Ytest)

# Print the results
print("Best Random Forest Model Parameters:", grid_search.best_params_)
print("Best Random Forest Model MAE:", mae_best)

#5.USE THE DATA FROM ANOTHER SEASON(PLAYER_22) WHICH WAS NOT USED DURING THE TRAINING TO TEST HOW GOOD IS THE MODEL.

"DONE!"



#6. DEPLOY THE MODEL ON A SIMPLE WEB PAGE USING EITHER (HEROKU,STREAMLINE,OR FLASK) AND UPLOAD A VIDEO THAT SHOWS HOW HOW THE MODEL PERFORMS  ON THE WEB PAGE/SITE.

"DONE!"



# pickling the model 
import pickle 
pickle_out = open("classifier.pkl", "wb") 
pickle.dump(xgb_model, pickle_out) 
pickle_out.close()
